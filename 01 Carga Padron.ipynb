{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feaba698",
   "metadata": {},
   "source": [
    "## findspark\n",
    "\n",
    "Si se está usando local, es conveniente usar `findspark` para poder utlizar la funcionalidad de spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48869f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import findspark\n",
    "findspark.init('/spark/spark-3.5.1-bin-hadoop3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3c89e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import *\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c2b191",
   "metadata": {},
   "source": [
    "## Sesión\n",
    "\n",
    "La sesión de Spark debe tener un nombre y es lo que une diferentes procesos de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7434c23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder.appName(\"padron\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76655bb",
   "metadata": {},
   "source": [
    "# RDD\n",
    "\n",
    "Las primeras versiones de Spark utilizaban los RDDs directamente.\n",
    "Usemos un ejemplo de contar palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cd0d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ejemplo.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54083baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ejemplo_RDD = spark.sparkContext.textFile(\"ejemplo.txt\").flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec2440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_map =  Ejemplo_RDD.map(lambda word: (word, 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79021752",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_reduce = rdd_map.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e00c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rdd_reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52834b31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resultado = rdd_reduce.collect()\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf85166",
   "metadata": {},
   "source": [
    "# Spark Dataframes\n",
    "\n",
    "Framework introducido por Spark 2+ para facilitar el manejo de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9063f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"./padron_completo/PADRON_COMPLETO.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffde1e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec466860",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ee5ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DateType\n",
    "from pyspark.sql.types import ArrayType, DoubleType, BooleanType\n",
    "\n",
    "padron_schema  = StructType(fields=[\n",
    "    StructField(\"CEDULA\",IntegerType(),True), \n",
    "    StructField(\"CODELEC\",IntegerType(),True),\n",
    "    StructField(\"RELLENO\",StringType(),True),\n",
    "    StructField(\"FECHACADUC\",DateType(),True),\n",
    "    StructField(\"JUNTA\",IntegerType(),True),\n",
    "    StructField(\"NOMBRE\",StringType(),True),\n",
    "    StructField(\"1_APELLIDO\",StringType(),True),\n",
    "    StructField(\"2_APELLIDO\",StringType(),True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9a7ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"dateFormat\", \"yyyyMMdd\")\\\n",
    "    .schema(padron_schema)\\\n",
    "    .csv(\"./padron_completo/PADRON_COMPLETO.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fdb570",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8198a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a01ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dfc879",
   "metadata": {},
   "outputs": [],
   "source": [
    "df .createOrReplaceTempView(\"padron\")\n",
    "results = spark.sql(\"SELECT * FROM padron WHERE FECHACADUC='2028-02-07'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b85d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
