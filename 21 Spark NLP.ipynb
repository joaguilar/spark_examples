{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9698d0be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/spark-3.5.1-bin-hadoop3')\n",
    "from pyspark import *\n",
    "# Import Spark NLP\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import sparknlp\n",
    "\n",
    "# Start SparkSession with Spark NLP\n",
    "# start() functions has 3 parameters: gpu, apple_silicon, and memory\n",
    "# sparknlp.start(gpu=True) #will start the session with GPU support\n",
    "spark = sparknlp.start(apple_silicon=True) # will start the session with macOS M1 & M2 support\n",
    "# sparknlp.start(memory=\"16G\") #to change the default driver memory in SparkSession\n",
    "# spark = sparknlp.start()\n",
    "# spark = SparkSession.builder.appName(\"analytics\").getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89f6abe9",
   "metadata": {},
   "source": [
    "# Preprocesamiento de Texto con Spark NLP\n",
    "\n",
    "Para realizar el preprocesamiento de texto, vamos a utilizar el modelo `explain_document_dl` ya preentrenado.\n",
    "\n",
    "Información sobre el modelo: [Explain Document DL Pipeline for English](https://www.johnsnowlabs.com/explain-document-pretrained-pipeline-spark-nlp-short-blogpost-series-1/)\n",
    "\n",
    "Este es un modelo \"sencillo\" que realiza las tareas de procesamiento de texto más comunes y reconocimiento de entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94315cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a pre-trained pipeline\n",
    "pipeline = PretrainedPipeline('explain_document_dl', lang='en')\n",
    "\n",
    "\n",
    "# Crear un conjunto de textos para analizar\n",
    "texts = [\n",
    "    \"Apple Inc. is looking to buy a startup in the United States.\",\n",
    "    \"Barack Obama was the 44th President of the United States.\",\n",
    "    \"Elon Musk founded SpaceX, an aerospace manufacturer and space transportation company.\",\n",
    "    \"The Amazon rainforest is the largest tropical rainforest in the world.\",\n",
    "    \"Google was founded by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University.\"\n",
    "]\n",
    "\n",
    "# Your testing dataset\n",
    "text = \"\"\"\n",
    "The Mona Lisa is a 16th century oil painting created by Leonardo.\n",
    "It's held at the Louvre in Paris.\n",
    "\"\"\"\n",
    "\n",
    "# Primero anotamos el texto de prueba que usa Spark NLP:\n",
    "result = pipeline.annotate(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff94f28",
   "metadata": {},
   "source": [
    "Usamos esta primera anotación para mostrar la información que extrae el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65614767",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What's in the pipeline\n",
    "list(result.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a220d8",
   "metadata": {},
   "source": [
    "Como podemos ver, el modelo tiene las siguientes capacidades:\n",
    "\n",
    "* `entities`: Extrae entidades del texto\n",
    "* `stem`: Realiza stemmming sobre el texto\n",
    "* `checked`:\n",
    "* `lemma`:\n",
    "* `document`:\n",
    "* `pos`:\n",
    "* `token`:\n",
    "* `ner`\n",
    "* `embeddings`:\n",
    "* `sentence`:\n",
    "\n",
    "Podemos iniciar mostrando las entidades retornadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac0e6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entidades:\n",
    "result['token']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf6a2d3",
   "metadata": {},
   "source": [
    "Antes de proseguir, creamos una funcion que nos pueda mostrar el texto junto con el resultado de estos analisis uno al lado del otro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68e9ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_dataframe(data, key):\n",
    "    # Ensure the key exists in the data\n",
    "    if key not in data:\n",
    "        raise ValueError(f\"The key '{key}' is not present in the data.\")\n",
    "    \n",
    "    # Create a DataFrame from the tokens and the specified key\n",
    "    df = pd.DataFrame({\n",
    "        'token': data['token'],\n",
    "        key: data[key]\n",
    "    })\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d8103b",
   "metadata": {},
   "source": [
    "Mostramos el resultado del *Part of Speech Tagger* Usando esta funcion. Como referencia, recuerden la clasificación que usa el POS Tagger:\n",
    "\n",
    "* CC coordinating conjunction\n",
    "* CD cardinal digit\n",
    "* DT determiner\n",
    "* EX existential there (like: “there is” … think of it like “there exists”)\n",
    "* FW foreign word\n",
    "* IN preposition/subordinating conjunction\n",
    "* JJ adjective \"big\"\n",
    "* JJR adjective, comparative \"bigger\"\n",
    "* JJS adjective, superlative \"biggest\"\n",
    "* LS list marker 1)\n",
    "* MD modal could, will\n",
    "* NN noun, singular \"desk\"\n",
    "* NNS noun plural \"desks\"\n",
    "* NNP proper noun, singular \"Harrison\"\n",
    "* NNPS proper noun, plural \"Americans\"\n",
    "* PDT predeterminer \"all the kids\"\n",
    "* POS possessive ending parent\"s\n",
    "* PRP personal pronoun I, he, she\n",
    "* PRP\\$ possessive pronoun my, his, hers\n",
    "* RB adverb very, silently,\n",
    "* RBR adverb, comparative better\n",
    "* RBS adverb, superlative best\n",
    "* RP particle give up\n",
    "* TO, to go \"to\" the store.\n",
    "* UH interjection, errrrrrrrm\n",
    "* VB verb, base form take\n",
    "* VBD verb, past tense took\n",
    "* VBG verb, gerund/present participle taking\n",
    "* VBN verb, past participle taken\n",
    "* VBP verb, sing. present, non-3d take\n",
    "* VBZ verb, 3rd person sing. present takes\n",
    "* WDT wh-determiner which\n",
    "* WP wh-pronoun who, what\n",
    "* WP\\$ possessive wh-pronoun whose\n",
    "* WRB wh-abverb where, when\n",
    "\n",
    "Tags tomadas de [Categorizing and POS Tagging with NLTK Python](https://www.learntek.org/blog/categorizing-pos-tagging-nltk-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4c303b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_dataframe(result,'pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd7a278",
   "metadata": {},
   "source": [
    "Ahora podemos ver como clasifica los tokens usando named entity recognition (NER), utilizando el formato *IOB (Inside, Outside, Beginning)*.\n",
    "\n",
    "Etiquetas IOB2:\n",
    "\n",
    "* `I- (Inside)`: Indica que el token está dentro de un chunk.\n",
    "* `O (Outside)`: Indica que el token no pertenece a ningún chunk.\n",
    "* `B- (Beginning)`: Indica que el token es el comienzo de un chunk\n",
    "\n",
    "Cuando un chunk comienza después de una etiqueta O, el primer token del chunk lleva el prefijo B-.\n",
    "\n",
    "\n",
    "Mas información sobre el formato IOB: [Inside–outside–beginning (tagging)](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f8c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataframe(result,'ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c3575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "El modelo también hace stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0bb62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results\n",
    "create_dataframe(result,'stem')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de61b892",
   "metadata": {},
   "source": [
    "Y lemmatización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d0fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results\n",
    "create_dataframe(result,'lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1460f70a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b62049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results\n",
    "result['token']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46a3226",
   "metadata": {},
   "source": [
    "## Analisis de Sentimiento\n",
    "\n",
    "Spark NLP contiene un módulo de análisis de sentimiento llamado [SentimentDetector](https://sparknlp.org/api/python/reference/autosummary/sparknlp/annotator/sentiment/sentiment_detector/index.html)\n",
    "\n",
    "Este módulo funciona en base a reglas, a diferencia de los modelos que vimos en la clase anterior que funcionan en base a modelos pre-entrenados. Estos modelos se basan en diccionarios de palabras que tienen connotaciones positivas o negativas, y en base a ellos determina mas una serie de heurísticas, determina si un texto es de sentimiento positivo o negativo.\n",
    "\n",
    "El siguiente ejemplo está basado en [Sentiment Analysis with Spark NLP without Machine Learning](https://www.johnsnowlabs.com/sentiment-analysis-with-spark-nlp-without-machine-learning/)\n",
    "\n",
    "Para este modelo, es necesario obtener primero los archivos de lemmatizacion y un diccionarque contiene palabras y su \"sentimiento\" asociado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea18d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -O --output-dir /tmp https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/lemma-corpus-small/lemmas_small.txt\n",
    "! curl -O --output-dir /tmp https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sentiment-corpus/default-sentiment-dict.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d49cf8a",
   "metadata": {},
   "source": [
    "El siguiente paso es crear el pipeline de Spark para analizar el texto (tomado de [Sentiment Analysis with Spark NLP without Machine Learning](https://www.johnsnowlabs.com/sentiment-analysis-with-spark-nlp-without-machine-learning/) ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5589cc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required modules and classes\n",
    "from sparknlp.base import DocumentAssembler, Pipeline, Finisher\n",
    "from sparknlp.annotator import (\n",
    "    SentenceDetector,\n",
    "    Tokenizer,\n",
    "    Lemmatizer,\n",
    "    SentimentDetector\n",
    ")\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Step 1: Transforms raw texts to `document` annotation\n",
    "document_assembler = (\n",
    "    DocumentAssembler()\n",
    "    .setInputCol(\"text\")\n",
    "    .setOutputCol(\"document\")\n",
    ")\n",
    "\n",
    "# Step 2: Sentence Detection\n",
    "sentence_detector = SentenceDetector().setInputCols([\"document\"]).setOutputCol(\"sentence\")\n",
    "\n",
    "# Step 3: Tokenization\n",
    "tokenizer = Tokenizer().setInputCols([\"sentence\"]).setOutputCol(\"token\")\n",
    "\n",
    "# Step 4: Lemmatization\n",
    "lemmatizer= Lemmatizer().setInputCols(\"token\").setOutputCol(\"lemma\")\\\n",
    "                        .setDictionary(\"/tmp/lemmas_small.txt\", key_delimiter=\"->\", value_delimiter=\"\\t\")\n",
    "\n",
    "# Step 5: Sentiment Detection\n",
    "sentiment_detector= (\n",
    "    SentimentDetector()\\\n",
    "    .setInputCols([\"lemma\", \"sentence\"])\\\n",
    "    .setOutputCol(\"sentiment_score\")\\\n",
    "    .setDictionary(\"/tmp/default-sentiment-dict.txt\", \",\")\n",
    ")\n",
    "\n",
    "# Step 6: Finisher\n",
    "finisher= (\n",
    "    Finisher()\n",
    "    .setInputCols([\"sentiment_score\"]).setOutputCols(\"sentiment\")\n",
    ")\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(\n",
    "    stages=[\n",
    "        document_assembler,\n",
    "        sentence_detector, \n",
    "        tokenizer, \n",
    "        lemmatizer, \n",
    "        sentiment_detector, \n",
    "        finisher\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4781a8af",
   "metadata": {},
   "source": [
    "Usamos los ejemplos de textos que utilizamos la clase anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ebe716",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    \"I think Alex Johnson is doing a fantastic job leading the country.\",\n",
    "    \"Alex Johnson's policies are ruining our economy.\",\n",
    "    \"I'm not sure about Alex Johnson's latest speech, it was confusing.\",\n",
    "    \"The new reforms introduced by Alex Johnson are very promising.\",\n",
    "    \"Alex Johnson seems to care about the people's issues, which is refreshing.\",\n",
    "    \"I am disappointed with Alex Johnson's performance.\",\n",
    "    \"Alex Johnson's leadership style is quite effective.\",\n",
    "    \"The way Alex Johnson handled the recent crisis was commendable.\",\n",
    "    \"I don't trust Alex Johnson's intentions at all.\",\n",
    "    \"Alex Johnson has brought positive changes to the healthcare system.\"\n",
    "]\n",
    "\n",
    "# Crear un DataFrame de Spark con la lista de frases\n",
    "tweets_df = spark.createDataFrame([(text,) for text in tweets], [\"text\"])\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "tweets_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff48b17",
   "metadata": {},
   "source": [
    "Y ejecutamos el pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c69026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit-transform to get predictions\n",
    "# Fit-transform to get predictions\n",
    "result = pipeline.fit(tweets_df).transform(tweets_df).show(truncate = 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Spark NLP)",
   "language": "python",
   "name": "sparknlp_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
