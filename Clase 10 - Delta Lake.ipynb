{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4298e17c-f782-4f42-9245-0ce3cbb06182",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import *\n",
    "\n",
    "# Crear sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeExample\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.7.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fb7541a-ccf1-4567-a114-c10b7c221557",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuración de rutas\n",
    "sourcePath = \"/databricks-datasets/learning-spark-v2/loans/loan-risks.snappy.parquet\"\n",
    "deltaPath = \"/tmp/loans_delta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce17529e-a715-4d26-a914-707e5743d35e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear tabla Delta Lake con datos de préstamos\n",
    "df = spark.read.format(\"parquet\").load(sourcePath)\n",
    "df.write.format(\"delta\").save(deltaPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12f55cf4-0520-4bfe-890f-e9e4f6dc8f33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear vista temporal sobre los datos\n",
    "spark.read.format(\"delta\").load(deltaPath).createOrReplaceTempView(\"loans_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f404bbdd-2036-4fb2-aea2-67b56907809d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(1)|\n+--------+\n|   14705|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Consultar los datos\n",
    "spark.sql(\"SELECT count(*) FROM loans_delta\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ee6367b4-f368-40e6-a5db-248a7be0c702",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99fdee2b-b94a-4e90-954f-28579e2ae549",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+\n|loan_id|funded_amnt|paid_amnt|addr_state|\n+-------+-----------+---------+----------+\n|      0|       1000|   182.22|        CA|\n|      1|       1000|   361.19|        WA|\n|      2|       1000|   176.26|        TX|\n|      3|       1000|   1000.0|        OK|\n|      4|       1000|   249.98|        PA|\n+-------+-----------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM loans_delta LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10006c42-4d16-4e96-81aa-bd62dd84d322",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- loan_id: long (nullable = true)\n |-- funded_amnt: integer (nullable = true)\n |-- paid_amnt: double (nullable = true)\n |-- addr_state: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ce37df7-9553-4089-bd86-15e37ab60eec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------------+----------+\n|loan_id|funded_amnt|         paid_amnt|addr_state|\n+-------+-----------+------------------+----------+\n|8275334|       1000| 536.4856148926217|        TX|\n|9971909|       1000|108.93329938258101|        NM|\n|2367400|       1000| 761.1484110892422|        OK|\n|7149065|       1000| 695.8854301025499|        TX|\n|6233620|       1000| 570.1251780892048|        CA|\n+-------+-----------+------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "import random, os\n",
    "\n",
    "# Crear un nuevo DataFrame de streaming con datos aleatorios\n",
    "streamingSourceDir = \"/tmp/streaming_source\"\n",
    "\n",
    "# Crear directorio de fuente de streaming si no existe\n",
    "if not os.path.exists(streamingSourceDir):\n",
    "    os.makedirs(streamingSourceDir)\n",
    "\n",
    "random_data = [(random.randint(1000000, 9999999), random.randint(1000, 1000), random.uniform(0, 1000), random.choice(['CA', 'WA', 'TX', 'OK', 'PA','NY','FL','NM'])) for _ in range(5)]\n",
    "schema = \"\"\"\n",
    "    loan_id LONG,\n",
    "    funded_amnt INT,\n",
    "    paid_amnt DOUBLE,\n",
    "    addr_state STRING\n",
    "\"\"\"\n",
    "newLoanStreamDF = spark.createDataFrame(random_data, schema=schema)\n",
    "newLoanStreamDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "815cb60d-db1c-453b-9300-855e5e8b1fa5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Guardar datos aleatorios en la carpeta de streaming\n",
    "newLoanStreamDF.write.mode(\"append\").json(streamingSourceDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acd5253e-8325-44fe-a99e-956fea351f56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear DataFrame de streaming leyendo de la carpeta\n",
    "streamingDF = spark.readStream.schema(schema).json(streamingSourceDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "386a16c0-5747-44c1-81e4-a767baa54547",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cargar datos de streaming en una tabla Delta Lake\n",
    "# Suponiendo que tenemos un DataFrame de streaming `newLoanStreamDF` con los mismos datos\n",
    "checkpointDir = \"/tmp/checkpoints\"\n",
    "streamingQuery = (streamingDF.writeStream\n",
    "                  .format(\"delta\")\n",
    "                  .option(\"checkpointLocation\", checkpointDir)\n",
    "                  .trigger(once=True)\n",
    "                  .start(deltaPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0db4998f-94c2-40e4-bf79-f232f152c79a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Esperar a que termine la consulta de streaming\n",
    "streamingQuery.awaitTermination()\n",
    "streamingQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "752076d1-673e-4e2d-8345-88c47997e57f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamingQuery.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e5dd7c8-f07a-4a53-9888-76a7b013aae4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(1)|\n+--------+\n|   14710|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Consultar los datos\n",
    "spark.sql(\"SELECT count(*) FROM loans_delta\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7007ffd3-d179-4870-96f6-631382abc5ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "option(\"mergeSchema\", \"true\"): Esta opción permite la evolución del esquema. Cuando se establece en \"true\", permite que Delta Lake realice cambios en el esquema de la tabla existente para acomodar nuevas columnas que puedan no estar presentes en el esquema actual. Es decir, si el DataFrame loanUpdates contiene columnas adicionales que no están en el esquema actual del Delta Lake, el esquema se actualizará automáticamente para incluir estas nuevas columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82ef3b64-dac8-476a-9d84-2b543f275264",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Forzar esquema en escritura para evitar corrupción de datos\n",
    "cols = ['loan_id', 'funded_amnt', 'paid_amnt', 'addr_state', 'closed']\n",
    "items = [(1111111, 1000, 1000.0, 'TX', True),\n",
    "         (2222222, 2000, 0.0, 'CA', False)]\n",
    "loanUpdates = spark.createDataFrame(items, cols)\n",
    "# Asegurar que los tipos de datos coincidan\n",
    "loanUpdates = loanUpdates.withColumn(\"loan_id\", col(\"loan_id\").cast(\"long\")) \\\n",
    "                         .withColumn(\"funded_amnt\", col(\"funded_amnt\").cast(\"int\")) \\\n",
    "                         .withColumn(\"paid_amnt\", col(\"paid_amnt\").cast(\"double\")) \\\n",
    "                         .withColumn(\"addr_state\", col(\"addr_state\").cast(\"string\"))\n",
    "\n",
    "loanUpdates.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(deltaPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3d91bb3-48c4-498d-9104-52bf2d547f2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema de la tabla actualizada:\nroot\n |-- loan_id: long (nullable = true)\n |-- funded_amnt: integer (nullable = true)\n |-- paid_amnt: double (nullable = true)\n |-- addr_state: string (nullable = true)\n |-- closed: boolean (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, deltaPath)\n",
    "# Upsert de datos: Fusionar cambios de datos nuevos\n",
    "deltaTable.alias(\"t\").merge(loanUpdates.alias(\"s\"), \"t.loan_id = s.loan_id\").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "# Ver esquema de la tabla actualizada\n",
    "print(\"Schema de la tabla actualizada:\")\n",
    "deltaTable.toDF().printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37542156-533c-4567-a955-71874a05ac92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de la tabla actualizada:\n+-------+-----------+---------+----------+------+\n|loan_id|funded_amnt|paid_amnt|addr_state|closed|\n+-------+-----------+---------+----------+------+\n|      0|       1000|   182.22|        CA|  NULL|\n|      1|       1000|   361.19|        WA|  NULL|\n|      2|       1000|   176.26|        TX|  NULL|\n|      3|       1000|   1000.0|        OK|  NULL|\n|      4|       1000|   249.98|        PA|  NULL|\n+-------+-----------+---------+----------+------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Mostrar los datos de la tabla actualizada\n",
    "print(\"Datos de la tabla actualizada:\")\n",
    "deltaTable.toDF().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f72d4bb0-b114-4693-9fb0-ddfcdbe3efb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filas con valor WA en addr_state\n",
    "deltaTable.toDF().filter(\"addr_state = 'WA'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ef58842-9043-45b9-aa5a-77647dc3c9ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filas con valor OR en addr_state\n",
    "deltaTable.toDF().filter(\"addr_state = 'OR'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b9d567e-2711-4dad-9570-9daceeaa5598",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transformar datos existentes: Actualizar estado de direcciones\n",
    "deltaTable.update(\"addr_state = 'OR'\", {\"addr_state\": \"'WA'\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35d59985-7e98-4825-9956-959030b4116c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "518"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filas con valor WA en addr_state\n",
    "deltaTable.toDF().filter(\"addr_state = 'WA'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ad58e1e-d0dc-4355-bf43-f33d1f258118",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filas con valor OR en addr_state\n",
    "deltaTable.toDF().filter(\"addr_state = 'OR'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a83dcf11-e4d2-4001-96bd-afc66e9452df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5134"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filas con \"funded_amnt =< paid_amnt\"\n",
    "deltaTable.toDF().filter(\"funded_amnt <= paid_amnt\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "395abc61-549a-45c5-8a15-389f20343d7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Eliminar datos de usuarios: Borrar préstamos completamente pagados\n",
    "deltaTable.delete(\"funded_amnt <= paid_amnt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aee4ba7-a56e-4a82-9bf5-b9adbc2adb36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filas con \"funded_amnt =< paid_amnt\"\n",
    "deltaTable.toDF().filter(\"funded_amnt <= paid_amnt\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5284b954-4194-4618-95ac-42eaad7df3ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9578"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filas totales\n",
    "deltaTable.toDF().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f4e564-78be-4a15-8350-dc32ad494732",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|version|timestamp          |operation       |operationParameters                                                                                                                                                                                            |\n+-------+-------------------+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|5      |2024-07-17 05:03:06|DELETE          |{predicate -> [\"(cast(funded_amnt#2428 as double) <= paid_amnt#2429)\"]}                                                                                                                                        |\n|4      |2024-07-17 05:00:47|UPDATE          |{predicate -> [\"(addr_state#2430 = OR)\"]}                                                                                                                                                                      |\n|3      |2024-07-17 04:56:37|MERGE           |{predicate -> [\"(loan_id#2427L = loan_id#1671L)\"], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}|\n|2      |2024-07-17 04:52:30|WRITE           |{mode -> Append, statsOnLoad -> false, partitionBy -> []}                                                                                                                                                      |\n|1      |2024-07-17 04:41:52|STREAMING UPDATE|{outputMode -> Append, queryId -> fb5e28de-15d6-4edc-b6d4-8d299a980903, epochId -> 0, statsOnLoad -> false}                                                                                                    |\n|0      |2024-07-17 04:28:44|WRITE           |{mode -> ErrorIfExists, statsOnLoad -> false, partitionBy -> []}                                                                                                                                               |\n+-------+-------------------+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Auditar cambios de datos: Mostrar historial de operaciones\n",
    "deltaTable.history().select(\"version\", \"timestamp\", \"operation\", \"operationParameters\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45179dc8-20eb-4ccd-b426-821eabfe3bc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+\n|loan_id|funded_amnt|paid_amnt|addr_state|\n+-------+-----------+---------+----------+\n|      0|       1000|   182.22|        CA|\n|      1|       1000|   361.19|        WA|\n|      2|       1000|   176.26|        TX|\n|      3|       1000|   1000.0|        OK|\n|      4|       1000|   249.98|        PA|\n+-------+-----------+---------+----------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Viajar en el tiempo: Consultar versiones anteriores de la tabla\n",
    "# De antes de que se hiciera el merge\n",
    "#spark.read.format(\"delta\").option(\"timestampAsOf\", \"2020-01-01\").load(deltaPath).show()\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", \"0\").load(deltaPath).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe05866f-7543-4b4d-af43-df5bf63e496a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+------+\n|loan_id|funded_amnt|paid_amnt|addr_state|closed|\n+-------+-----------+---------+----------+------+\n|      0|       1000|   182.22|        CA|  NULL|\n|      1|       1000|   361.19|        WA|  NULL|\n|      2|       1000|   176.26|        TX|  NULL|\n|      3|       1000|   1000.0|        OK|  NULL|\n|      4|       1000|   249.98|        PA|  NULL|\n+-------+-----------+---------+----------+------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Version despues del merge\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", \"3\").load(deltaPath).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7398e3d-b1b7-4160-bbb0-a7acf8bb5041",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Custom TB Handler failed, unregistering\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1919\u001B[0m, in \u001B[0;36mSparkSession.stop\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1918\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1919\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSparkSession\u001B[49m\u001B[38;5;241m.\u001B[39mclearDefaultSession()\n\u001B[1;32m   1920\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSparkSession\u001B[38;5;241m.\u001B[39mclearActiveSession()\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1758\u001B[0m, in \u001B[0;36mJVMView.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1756\u001B[0m message \u001B[38;5;241m=\u001B[39m compute_exception_message(\n\u001B[1;32m   1757\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m does not exist in the JVM\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(name), error_message)\n\u001B[0;32m-> 1758\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(message)\n\n\u001B[0;31mPy4JError\u001B[0m: SparkSession does not exist in the JVM -- org.apache.spark.SparkException: Trying to putInheritedProperty with no active spark context\n\tat org.apache.spark.credentials.CredentialContext$.$anonfun$putInheritedProperty$2(CredentialContext.scala:188)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.credentials.CredentialContext$.$anonfun$putInheritedProperty$1(CredentialContext.scala:188)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.credentials.CredentialContext$.putInheritedProperty(CredentialContext.scala:187)\n\tat com.databricks.backend.daemon.driver.SparkThreadLocalUtils$$anon$1.$anonfun$run$2(SparkThreadLocalUtils.scala:57)\n\tat com.databricks.backend.daemon.driver.SparkThreadLocalUtils$$anon$1.$anonfun$run$2$adapted(SparkThreadLocalUtils.scala:57)\n\tat scala.Option.foreach(Option.scala:407)\n\tat com.databricks.backend.daemon.driver.SparkThreadLocalUtils$$anon$1.run(SparkThreadLocalUtils.scala:57)\n\tat java.lang.Iterable.forEach(Iterable.java:75)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:198)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\nDuring handling of the above exception, another exception occurred:\n\n\u001B[0;31mPy4JError\u001B[0m                                 Traceback (most recent call last)\n    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n\nFile \u001B[0;32m<command-1810546028644299>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Detener la sesión de Spark\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:53\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m ex:\n\u001B[0;32m---> 53\u001B[0m     \u001B[43mlogger\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlog_failure\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclass_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunction_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[43m        \u001B[49m\u001B[43mex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mperf_counter\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mstart\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[43m        \u001B[49m\u001B[43msignature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/databricks/usage_logger.py:70\u001B[0m, in \u001B[0;36mUsageLogger.log_failure\u001B[0;34m(self, module_name, class_name, name, ex, duration, signature)\u001B[0m\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogger\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecordFunctionCallFailureEvent\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     71\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     72\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclass_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[43m    \u001B[49m\u001B[43msignature\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mand\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msignature\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     75\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mex\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     76\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mex\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[43m    \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdumps\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mduration\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mduration\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     78\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:224\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 224\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:330\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    329\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 330\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\n\u001B[0;31mPy4JError\u001B[0m: An error occurred while calling o367.recordFunctionCallFailureEvent. Trace:\norg.apache.spark.SparkException: Trying to putInheritedProperty with no active spark context\n\tat org.apache.spark.credentials.CredentialContext$.$anonfun$putInheritedProperty$2(CredentialContext.scala:188)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.credentials.CredentialContext$.$anonfun$putInheritedProperty$1(CredentialContext.scala:188)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.credentials.CredentialContext$.putInheritedProperty(CredentialContext.scala:187)\n\tat com.databricks.backend.daemon.driver.SparkThreadLocalUtils$$anon$1.$anonfun$run$2(SparkThreadLocalUtils.scala:57)\n\tat com.databricks.backend.daemon.driver.SparkThreadLocalUtils$$anon$1.$anonfun$run$2$adapted(SparkThreadLocalUtils.scala:57)\n\tat scala.Option.foreach(Option.scala:407)\n\tat com.databricks.backend.daemon.driver.SparkThreadLocalUtils$$anon$1.run(SparkThreadLocalUtils.scala:57)\n\tat java.lang.Iterable.forEach(Iterable.java:75)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:198)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\n\nDuring handling of the above exception, another exception occurred:\n\n\u001B[0;31mPy4JError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m/databricks/python_shell/dbruntime/ExceptionHandler.py:33\u001B[0m, in \u001B[0;36mcustom_exception_handler\u001B[0;34m(shell, etype, exception, tb, tb_offset)\u001B[0m\n\u001B[1;32m     30\u001B[0m     shell\u001B[38;5;241m.\u001B[39mpayload_manager\u001B[38;5;241m.\u001B[39mwrite_payload({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msource\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m'\u001B[39m: filter_None(data)})\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# This will call shell._showtraceback, which we patch in DatabricksShell.py to truncate the error message.\u001B[39;00m\n\u001B[0;32m---> 33\u001B[0m \u001B[43mshell\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowtraceback\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexception\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb_offset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_offset\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2116\u001B[0m, in \u001B[0;36mInteractiveShell.showtraceback\u001B[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[1;32m   2113\u001B[0m     traceback\u001B[38;5;241m.\u001B[39mprint_exc()\n\u001B[1;32m   2114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2116\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_showtraceback\u001B[49m\u001B[43m(\u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2117\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcall_pdb:\n\u001B[1;32m   2118\u001B[0m     \u001B[38;5;66;03m# drop into debugger\u001B[39;00m\n\u001B[1;32m   2119\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdebugger(force\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/DatabricksShell.py:73\u001B[0m, in \u001B[0;36mDatabricksShell._showtraceback\u001B[0;34m(self, etype, evalue, stb)\u001B[0m\n\u001B[1;32m     71\u001B[0m full_evalue \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(evalue)\n\u001B[1;32m     72\u001B[0m \u001B[38;5;66;03m# [ES-1024635]: error message can get so long it crashes the driver.\u001B[39;00m\n\u001B[0;32m---> 73\u001B[0m truncated_evalue \u001B[38;5;241m=\u001B[39m truncate(full_evalue, limit\u001B[38;5;241m=\u001B[39m\u001B[43mget_max_error_message_length\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m# The traceback contains the error message too, but it contains ansi escape characters for\u001B[39;00m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;66;03m# coloring, so we find/replace the full error message with the truncated one.\u001B[39;00m\n\u001B[1;32m     76\u001B[0m stb \u001B[38;5;241m=\u001B[39m [line\u001B[38;5;241m.\u001B[39mreplace(full_evalue, truncated_evalue) \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m stb]\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/utils.py:104\u001B[0m, in \u001B[0;36mget_max_error_message_length\u001B[0;34m(shell)\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_max_error_message_length\u001B[39m(shell) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[1;32m    102\u001B[0m     default \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m100_000\u001B[39m\n\u001B[1;32m    103\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mint\u001B[39m(\n\u001B[0;32m--> 104\u001B[0m         \u001B[43mshell\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspark_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mspark.databricks.driver.ipykernel.maxErrorMessageLength\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    105\u001B[0m \u001B[43m                               \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdefault\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(shell, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark_config\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m default)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/conf.py:244\u001B[0m, in \u001B[0;36mSparkConf.get\u001B[0;34m(self, key, defaultValue)\u001B[0m\n\u001B[1;32m    242\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    243\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jconf \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 244\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jconf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdefaultValue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    245\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    246\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conf \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:224\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    223\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 224\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    225\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    226\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:330\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    326\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 330\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    334\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    335\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    336\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name))\n\n\u001B[0;31mPy4JError\u001B[0m: An error occurred while calling o357.get. Trace:\norg.apache.spark.SparkException: Trying to putInheritedProperty with no active spark context\n\tat org.apache.spark.credentials.CredentialContext$.$anonfun$putInheritedProperty$2(CredentialContext.scala:188)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.credentials.CredentialContext$.$anonfun$putInheritedProperty$1(CredentialContext.scala:188)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.credentials.CredentialContext$.putInheritedProperty(CredentialContext.scala:187)\n\tat com.databricks.backend.daemon.driver.SparkThreadLocalUtils$$anon$1.$anonfun$run$2(SparkThreadLocalUtils.scala:57)\n\tat com.databricks.backend.daemon.driver.SparkThreadLocalUtils$$anon$1.$anonfun$run$2$adapted(SparkThreadLocalUtils.scala:57)\n\tat scala.Option.foreach(Option.scala:407)\n\tat com.databricks.backend.daemon.driver.SparkThreadLocalUtils$$anon$1.run(SparkThreadLocalUtils.scala:57)\n\tat java.lang.Iterable.forEach(Iterable.java:75)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:198)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\nThe original exception:\n"
     ]
    }
   ],
   "source": [
    "# Detener la sesión de Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Clase 10",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
