{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9698d0be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/spark-3.5.1-bin-hadoop3')\n",
    "from pyspark import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, col\n",
    "from pyspark.sql.types import StructType, StructField, FloatType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"analytics\").config(\"spark.driver.memory\", \"2g\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f66a6b",
   "metadata": {},
   "source": [
    "# Streaming K-Means\n",
    "\n",
    "\n",
    "### ¿En qué escenarios se puede aplicar?\n",
    "\n",
    "* Deteccion de fraudes monitoreando transacciones\n",
    "* Deteccion de anomalias en transito de red\n",
    "* Monitoreo de sensores de IoT\n",
    "\n",
    "## Generación de Datos sintéticos:\n",
    "\n",
    "Para poder simular el flujo de información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469836c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Definimos el esquema del dataset simulado\n",
    "schema = StructType([\n",
    "    StructField(\"age\", FloatType(), True),\n",
    "    StructField(\"creatinine_phosphokinase\", FloatType(), True),\n",
    "    StructField(\"ejection_fraction\", FloatType(), True),\n",
    "    StructField(\"platelets\", FloatType(), True),\n",
    "    StructField(\"serum_creatinine\", FloatType(), True),\n",
    "    StructField(\"serum_sodium\", FloatType(), True),\n",
    "    StructField(\"time\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Simulamos un stream de datos creando un DataFrame con valores aleatorios\n",
    "def generate_stream_data():\n",
    "    return spark.range(0, 100).select(\n",
    "        (rand() * 100).cast(\"float\").alias(\"age\"),\n",
    "        (rand() * 8000).cast(\"float\").alias(\"creatinine_phosphokinase\"),\n",
    "        (rand() * 80).cast(\"float\").alias(\"ejection_fraction\"),\n",
    "        (rand() * 450000).cast(\"float\").alias(\"platelets\"),\n",
    "        (rand() * 5).cast(\"float\").alias(\"serum_creatinine\"),\n",
    "        (rand() * 150).cast(\"float\").alias(\"serum_sodium\"),\n",
    "        (rand() * 300).cast(\"float\").alias(\"time\")\n",
    "    )\n",
    "\n",
    "# Generamos datos de entrenamiento iniciales\n",
    "initial_data = generate_stream_data()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99283a97",
   "metadata": {},
   "source": [
    "## Procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d36ccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos las características que vamos a utilizar para el clustering\n",
    "features = ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']\n",
    "\n",
    "# Vectorizamos las características seleccionadas\n",
    "vector_assembler = VectorAssembler(inputCols=features, outputCol=\"unscaled_features\")\n",
    "initial_data_vector = vector_assembler.transform(initial_data)\n",
    "\n",
    "# Escalamos las características\n",
    "scaler = StandardScaler(inputCol=\"unscaled_features\", outputCol=\"features\")\n",
    "scaler_model = scaler.fit(initial_data_vector)\n",
    "initial_data_scaled = scaler_model.transform(initial_data_vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f34df2",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a560ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el modelo KMeans inicial\n",
    "kmeans = KMeans(k=3, seed=1, featuresCol=\"features\")\n",
    "model = kmeans.fit(initial_data_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92da70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para transformar y escalar nuevos datos del stream\n",
    "def transform_and_scale(df):\n",
    "    df_vector = vector_assembler.transform(df)\n",
    "    return scaler_model.transform(df_vector)\n",
    "\n",
    "# Simulamos la generación de datos de stream y los escribimos a una carpeta\n",
    "for i in range(2):\n",
    "    stream_data = generate_stream_data()\n",
    "    stream_data.write.mode(\"append\").parquet(\"data/simulated_stream\")\n",
    "    time.sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba661c1a",
   "metadata": {},
   "source": [
    "## Modelo de Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a498f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos el stream de datos desde la carpeta\n",
    "streaming_data = spark.readStream.schema(schema).parquet(\"data/simulated_stream\")\n",
    "\n",
    "# Transformamos y escalamos los datos del stream\n",
    "streaming_data_transformed = transform_and_scale(streaming_data)\n",
    "\n",
    "# Realizamos el clustering en el stream de datos\n",
    "streaming_predictions = model.transform(streaming_data_transformed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa8befc",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "Ejecucion del `StreamingQuery` para refrescar los datos como se generan.\n",
    "\n",
    "Ejecutar el notebook \"18-2\" una vez que se ejecuta esta celda para que agregue datos al archivo que estamos utilizando para simular el streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf7253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Función para imprimir los centroides actuales\n",
    "def print_centroids():\n",
    "    centers = model.clusterCenters()\n",
    "    print(\"Cluster Centers: \")\n",
    "    for center in centers:\n",
    "        print(center)\n",
    "\n",
    "# Realizamos el clustering en el stream de datos y actualizamos el modelo\n",
    "def update_and_print_clusters(df, epoch_id):\n",
    "    global model\n",
    "    df.persist()\n",
    "    if not df.isEmpty():\n",
    "        # Actualizamos el modelo con los nuevos datos del batch\n",
    "        model = kmeans.fit(df)\n",
    "        # Realizamos el clustering en el batch actual\n",
    "        predictions = model.transform(df)\n",
    "        # Imprimimos los centroides actuales\n",
    "        centers = model.clusterCenters()\n",
    "        print(f\"Epoch {epoch_id} Cluster Centers: \")\n",
    "        for center in centers:\n",
    "            print(center)\n",
    "        # Mostramos los resultados del clustering\n",
    "#         predictions.show()\n",
    "    df.unpersist()        \n",
    "        \n",
    "print(\"Centroides iniciales:\")\n",
    "print_centroids()\n",
    "        \n",
    "# Mostramos los resultados del clustering en tiempo real\n",
    "# query = streaming_predictions.writeStream \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .trigger(processingTime='20 seconds') \\\n",
    "#     .foreachBatch(lambda df, epoch_id: print_centroids()) \\\n",
    "#     .start()\n",
    "query = streaming_data_transformed.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='30 seconds') \\\n",
    "    .foreachBatch(update_and_print_clusters) \\\n",
    "    .start()\n",
    "\n",
    "# Esperamos a que el stream termine\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d8e81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
