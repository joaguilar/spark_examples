{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9698d0be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/spark-3.5.1-bin-hadoop3')\n",
    "from pyspark import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.sql.types import StructType, StructField, FloatType\n",
    "from pyspark.mllib.clustering import StreamingKMeans\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.streaming import StreamingContext\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"analytics\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Configuramos el contexto de streaming con un intervalo de 1 segundo\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa8f57c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/bin/python3\r\n"
     ]
    }
   ],
   "source": [
    "!which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1540579a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e5633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "# Iniciamos la sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"analytics\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Definimos el esquema del dataset simulado\n",
    "schema = StructType([\n",
    "    StructField(\"age\", FloatType(), True),\n",
    "    StructField(\"creatinine_phosphokinase\", FloatType(), True),\n",
    "    StructField(\"ejection_fraction\", FloatType(), True),\n",
    "    StructField(\"platelets\", FloatType(), True),\n",
    "    StructField(\"serum_creatinine\", FloatType(), True),\n",
    "    StructField(\"serum_sodium\", FloatType(), True),\n",
    "    StructField(\"time\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Simulamos un dataset para entrenar el modelo de escalado\n",
    "def generate_training_data():\n",
    "    return spark.range(0, 1000).select(\n",
    "        (rand() * 100).alias(\"age\"),\n",
    "        (rand() * 8000).alias(\"creatinine_phosphokinase\"),\n",
    "        (rand() * 80).alias(\"ejection_fraction\"),\n",
    "        (rand() * 450000).alias(\"platelets\"),\n",
    "        (rand() * 5).alias(\"serum_creatinine\"),\n",
    "        (rand() * 150).alias(\"serum_sodium\"),\n",
    "        (rand() * 300).alias(\"time\")\n",
    "    )\n",
    "\n",
    "training_data = generate_training_data()\n",
    "\n",
    "# Seleccionamos las características que vamos a utilizar para el escalado\n",
    "features = ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time']\n",
    "\n",
    "# Vectorizamos las características seleccionadas\n",
    "vector_assembler = VectorAssembler(inputCols=features, outputCol=\"unscaled_features\")\n",
    "training_data_vector = vector_assembler.transform(training_data)\n",
    "\n",
    "# Entrenamos el StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"unscaled_features\", outputCol=\"features\")\n",
    "scaler_model = scaler.fit(training_data_vector)\n",
    "\n",
    "# Guardamos el modelo de escalado\n",
    "scaler_model.write().overwrite().save(\"data/scaler_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6804647d",
   "metadata": {},
   "source": [
    "# Streaming K-Means\n",
    "\n",
    "### ¿Qué es y para qué se usa Streaming K-Means?\n",
    "**Streaming K-Means** es una variante del algoritmo de clustering K-Means diseñado para trabajar con datos que llegan en un flujo continuo (streaming). A diferencia del K-Means tradicional, que opera en un conjunto de datos estático, Streaming K-Means permite actualizar los centroides de los clusters en tiempo real conforme nuevos datos llegan, sin necesidad de almacenar todos los datos en memoria. Esto es particularmente útil en aplicaciones donde los datos son generados de forma continua, como monitoreo de sensores, análisis de tráfico de redes, procesamiento de logs en tiempo real, entre otros.\n",
    "\n",
    "### ¿Cómo funciona?\n",
    "El funcionamiento de Streaming K-Means se basa en actualizar iterativamente los centroides de los clusters a medida que nuevos datos son procesados. Uno de los parámetros clave en este proceso es el decayFactor, que controla la influencia de los datos nuevos en la actualización de los centroides. El decayFactor determina la tasa de olvido, es decir, cuánto peso se da a los datos recientes en comparación con los datos históricos. Un decayFactor cercano a 1 implica que los datos nuevos tienen un peso significativo y los centroides se adaptan rápidamente a los cambios recientes, mientras que un valor cercano a 0 implica que los datos históricos tienen más peso y los centroides cambian más lentamente. Este balance permite ajustar el modelo según las necesidades específicas de la aplicación, ya sea para captar rápidamente cambios en los patrones o para mantener estabilidad en clusters bien definidos.\n",
    "\n",
    "### ¿En qué escenarios se puede aplicar?\n",
    "\n",
    "* Deteccion de fraudes monitoreando transacciones\n",
    "* Deteccion de anomalias en transito de red\n",
    "* Monitoreo de sensores de IoT\n",
    "\n",
    "## Generación de Datos sintéticos:\n",
    "\n",
    "Para poder simular el flujo de información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ea8237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el esquema del dataset simulado\n",
    "schema = StructType([\n",
    "    StructField(\"age\", FloatType(), True),\n",
    "    StructField(\"creatinine_phosphokinase\", FloatType(), True),\n",
    "    StructField(\"ejection_fraction\", FloatType(), True),\n",
    "    StructField(\"platelets\", FloatType(), True),\n",
    "    StructField(\"serum_creatinine\", FloatType(), True),\n",
    "    StructField(\"serum_sodium\", FloatType(), True),\n",
    "    StructField(\"time\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Simulamos un stream de datos creando un DataFrame con valores aleatorios\n",
    "def generate_stream_data():\n",
    "    return spark.range(0, 100).select(\n",
    "        (rand() * 100).alias(\"age\"),\n",
    "        (rand() * 8000).alias(\"creatinine_phosphokinase\"),\n",
    "        (rand() * 80).alias(\"ejection_fraction\"),\n",
    "        (rand() * 450000).alias(\"platelets\"),\n",
    "        (rand() * 5).alias(\"serum_creatinine\"),\n",
    "        (rand() * 150).alias(\"serum_sodium\"),\n",
    "        (rand() * 300).alias(\"time\")\n",
    "    )\n",
    "\n",
    "# Entrenamos el StandardScaler y guardamos el modelo\n",
    "def train_and_save_scaler_model():\n",
    "    training_data = generate_stream_data()\n",
    "    vector_assembler = VectorAssembler(inputCols=['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time'], outputCol=\"unscaled_features\")\n",
    "    training_data_vector = vector_assembler.transform(training_data)\n",
    "    scaler = StandardScaler(inputCol=\"unscaled_features\", outputCol=\"features\")\n",
    "    scaler_model = scaler.fit(training_data_vector)\n",
    "    scaler_model.write().overwrite().save(\"data/scaler_model\")\n",
    "    return scaler_model\n",
    "\n",
    "scaler_model = train_and_save_scaler_model()\n",
    "\n",
    "# Inicializamos el VectorAssembler y cargamos el modelo de escalado\n",
    "vector_assembler = VectorAssembler(inputCols=['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time'], outputCol=\"unscaled_features\")\n",
    "scaler_model = StandardScaler.load('data/scaler_model')\n",
    "\n",
    "# Función para simular la generación de datos en un DStream\n",
    "def stream_data_generator():\n",
    "    while True:\n",
    "        new_data = generate_stream_data()\n",
    "        yield new_data\n",
    "\n",
    "        \n",
    "        \n",
    "# Generamos un DStream simulado\n",
    "input_data = ssc.queueStream([sc.parallelize(generate_stream_data().select(\"age\", \"creatinine_phosphokinase\", \"ejection_fraction\", \"platelets\", \"serum_creatinine\", \"serum_sodium\", \"time\").collect()) for _ in range(5)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e4d138",
   "metadata": {},
   "source": [
    "## Procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901d0afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformamos los datos usando VectorAssembler y StandardScalerModel directamente\n",
    "def transform(rdd):\n",
    "    if not rdd.isEmpty():\n",
    "        df = rdd.toDF(schema)\n",
    "        df_vector = vector_assembler.transform(df)\n",
    "        df_scaled = scaler_model.transform(df_vector)\n",
    "        return df_scaled.select(\"features\").rdd.map(lambda row: DenseVector(row[\"features\"]))\n",
    "    else:\n",
    "        return rdd\n",
    "\n",
    "data_stream = input_data.transform(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d534f",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df07f3aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa282fba",
   "metadata": {},
   "source": [
    "## StreamingKMeans\n",
    "\n",
    "Definimos el modelo StreamingKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3cba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el modelo StreamingKMeans\n",
    "initCenters = [[0.0] * 7, [1.0] * 7, [2.0] * 7]\n",
    "initWeights = [1.0, 1.0, 1.0]\n",
    "streaming_kmeans = StreamingKMeans(k=3, decayFactor=0.5, timeUnit=\"points\").setInitialCenters(initCenters, initWeights)\n",
    "\n",
    "# Entrenamos el modelo inicial con el DStream\n",
    "streaming_kmeans.trainOn(data_stream)\n",
    "\n",
    "# Función para mostrar los centroides actuales\n",
    "def print_centroids(model):\n",
    "    centers = model.latestModel().centers\n",
    "    print(\"Cluster Centers: \")\n",
    "    for center in centers:\n",
    "        print(center)\n",
    "\n",
    "# Mostramos los centroides iniciales\n",
    "print(\"Initial cluster centers:\")\n",
    "print_centroids(streaming_kmeans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b0f5c7",
   "metadata": {},
   "source": [
    "## Actualización del modelo\n",
    "\n",
    "Con el primer \"batch\" de datos que llega por la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff68a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciamos el contexto de streaming\n",
    "ssc.start()\n",
    "time.sleep(10)  # Esperamos un tiempo para permitir que los datos se procesen\n",
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)\n",
    "\n",
    "# Mostramos los centroides finales\n",
    "print(\"Final cluster centers:\")\n",
    "print_centroids(streaming_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae32762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos los centroides finales\n",
    "print(\"Final cluster centers:\")\n",
    "print_centroids(streaming_kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ddae22",
   "metadata": {},
   "source": [
    "## Simulación del proceso de actualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161323e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos los centroides iniciales\n",
    "print(\"Initial cluster centers:\")\n",
    "print_centroids(model)\n",
    "\n",
    "# Función para actualizar el modelo con nuevos datos en el stream y mostrar los centroides\n",
    "def update_model_and_print_centroids():\n",
    "    new_data = generate_stream_data()\n",
    "    new_data_scaled = pipeline_model.transform(new_data)\n",
    "    model.trainOn(new_data_scaled)\n",
    "    print(\"Updated cluster centers:\")\n",
    "    print_centroids(model)\n",
    "\n",
    "# Simulamos la actualización del modelo con nuevos datos\n",
    "for _ in range(5):  # Simulamos 5 batches de datos\n",
    "    update_model_and_print_centroids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2359da",
   "metadata": {},
   "source": [
    "## Evaluación del modelo\n",
    "\n",
    "* Realiza predicciones en el conjunto de prueba.\n",
    "* Evalúa el modelo utilizando la métrica de RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1259aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer predicciones\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluar el modelo\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) en el conjunto de prueba: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406e1e05",
   "metadata": {},
   "source": [
    "## Guardar y cargar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f48a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo\n",
    "pipelinePath = \"data/lr-pipeline-model\"\n",
    "pipelineModel.write().overwrite().save(pipelinePath)\n",
    "\n",
    "\n",
    "# Cargar el modelo\n",
    "from pyspark.ml import PipelineModel \n",
    "savedPipelineModel = PipelineModel.load(pipelinePath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
